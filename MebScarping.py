from playwright.async_api import async_playwright
import asyncio
import pandas as pd  # Pandas'Ä± ekledik


class MebPageScraper:
    def __init__(self, browser_path: str, url: str, row_number: int = 5):
        self.browser_path = browser_path
        self.url = url
        self.row_number = row_number

    async def fetch_page_content(self):
        async with async_playwright() as p:
            try:
                print("ğŸ”¹ Edge tarayÄ±cÄ±sÄ± baÅŸlatÄ±lÄ±yor...")
                browser = await p.chromium.launch(executable_path=self.browser_path, headless=False)
                page = await browser.new_page()

                print("ğŸ”¹ Sayfa aÃ§Ä±lÄ±yor...")
                await page.goto(self.url, timeout=120000)
                await asyncio.sleep(1)

                print("ğŸ”¹ Kurum tÃ¼rÃ¼ seÃ§iliyor...")
                await page.fill('input#cmbKurumTuru_I', 'Ã–zel Kurumlar')
                await asyncio.sleep(1)

                print("ğŸ”¹ Ä°stanbul seÃ§iliyor...")
                await page.click('input#cmbil_I')
                await asyncio.sleep(1)

                await page.evaluate("""
                    const inputElement = document.querySelector('input[name="cmbil"]');
                    inputElement.value = 'Ä°STANBUL';
                    const event = new Event('change');
                    inputElement.dispatchEvent(event);
                """)
                await asyncio.sleep(1)

                print("ğŸ”¹ Kurum listeleme butonuna tÄ±klanÄ±yor...")
                await page.evaluate("document.querySelector('#btnKurumListelex_I').click()")
                await asyncio.sleep(5)  # Ekstra bekleme sÃ¼resi ekledik

                print("ğŸ”¹ Sayfa yÃ¼kleniyor, lÃ¼tfen bekleyin...")
                await page.wait_for_load_state('networkidle', timeout=180000)

                print("âœ… Kurum listesini Ã§ekiyorum...")
                await asyncio.sleep(3)  # Ekstra bekleme (sayfanÄ±n tÃ¼m iÃ§eriÄŸinin yÃ¼klenmesi iÃ§in)

                rows = await page.query_selector_all("tr.dxgvDataRow_DevEx")
                print(f"âœ… Bulunan satÄ±r sayÄ±sÄ±: {len(rows)}")

                if not rows:
                    print("âŒ HATA: HiÃ§ satÄ±r bulunamadÄ±!")
                    await browser.close()
                    return pd.DataFrame()

                kurumlar_listesi = []

                for i, row in enumerate(rows[:self.row_number]):
                    cells = await row.query_selector_all("td")

                    if not cells:
                        print(f"âŒ HATA: SatÄ±r {i} iÃ§inde hiÃ§ <td> yok!")
                        continue

                    row_data = [await cell.evaluate("node => node.textContent.trim()") for cell in cells]

                    # Ä°lk boÅŸ hÃ¼creyi satÄ±r numarasÄ±yla doldur
                    row_data[0] = i + 1

                    kurumlar_listesi.append(row_data)
                    print(f"âœ… SatÄ±r {i} Verileri: {row_data}")

                await browser.close()

                # SÃ¼tun adlarÄ±nÄ± gÃ¼ncelleyelim: Her satÄ±r iÃ§in benzersiz bir ID (1, 2, 3...) ekle
                columns = ["ID", "Ä°l", "Ä°lÃ§e", "Kurum AdÄ±", "Kurum TÃ¼rÃ¼", "Adres", "Telefon 1", "Telefon 2",
                           "Kurum Kodu", "Web Sitesi"]

                # Verileri DataFrame'e ekleyelim
                df = pd.DataFrame(kurumlar_listesi, columns=columns)

                return df

            except Exception as e:
                print(f"âŒ Bir hata oluÅŸtu: {e}")
                return pd.DataFrame()


async def main():
    browser_path = r"C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe"
    url = "https://mebbis.meb.gov.tr/kurumlistesi.aspx"

    scraper = MebPageScraper(browser_path, url)
    df_kurumlar = await scraper.fetch_page_content()

    if not df_kurumlar.empty:
        print("\nğŸ”¹ Ä°lk 5 Kurum:")
        print(df_kurumlar)

        # CSV olarak kaydetmek istersen:
        df_kurumlar.to_csv("kurumlar_listesi.csv", index=False, encoding="utf-8-sig")


